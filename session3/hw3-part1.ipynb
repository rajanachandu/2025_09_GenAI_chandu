{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d092ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Create a class `LLMModel` with attributes `name` and `params`. Add a method `describe()` \n",
    "# that returns a string about the model. Create an object for `Mistral-7B` and print its description.\n",
    "#from os import name\n",
    "#from pydoc import describe\n",
    "\n",
    "\n",
    "class LLMModel:\n",
    "    def __init__(self,name,__params):\n",
    "        self.name=name\n",
    "        self.__params=__params\n",
    "\n",
    "    def describe(self):\n",
    "        return f\"The model is {self.name}, a powerful and efficient open-source language model with 7 billion parameters, \" \\\n",
    "        \"developed by Mistral AI.\"\n",
    "\n",
    "    def get_params_private(self):\n",
    "        return self.__params\n",
    "\n",
    "\n",
    "Mistral = LLMModel(\"Mistral-7B\",\"7B\")\n",
    "print(Mistral.describe())\n",
    "print(Mistral.name)\n",
    "print(Mistral.get_params_private())\n",
    "#print(Mistral.__params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c04063a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Create a class `TokenCounter`: - Use a private variable `__count`. - Add a method `add_tokens(num)` to \n",
    "# increase the count. - Add a method `get_count()` to return the count. - Demonstrate its usage.\n",
    "\n",
    "class TokenCounter:\n",
    "    def __init__(self,__count):\n",
    "        self.__count=__count\n",
    "\n",
    "    def add_tokens(self,num):\n",
    "        self.__count+=num\n",
    "\n",
    "    def get_count_private(self):\n",
    "        return self.__count\n",
    "\n",
    "    \n",
    "tokenCounter=TokenCounter(0)\n",
    "tokenCounter.add_tokens(10)\n",
    "\n",
    "print(tokenCounter.get_count_private())\n",
    "#print(tokenCounter.__count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed160b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.Create a base class `ChatBot` with a `reply(message)` method. Derive a class `MistralChatBot` that overrides \n",
    "#`reply(message)`. Show how inheritance works here.\n",
    "\n",
    "\n",
    "class ChatBot:\n",
    "    def __init__(self,name):\n",
    "        self.name=name\n",
    "\n",
    "    def reply(self,message):\n",
    "        return self.name + ' says ' + message\n",
    "\n",
    "\n",
    "class MistralChatBot(ChatBot):\n",
    "    def reply(self,message):\n",
    "        return self.name +  f\" says, {message[::-1]}\"\n",
    "\n",
    "\n",
    "chatBot = ChatBot('ChatBot')\n",
    "print(chatBot.reply('Hello'))\n",
    "\n",
    "mistralChatBot = MistralChatBot('MistralChatBot')\n",
    "print(mistralChatBot.reply('Hello LLM'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a11d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.Demonstrate polymorphism by creating two classes `OpenAIModel` and `OllamaModel` with the same method `run(prompt)`. \n",
    "# Write a function `test_model(model, prompt)` that can accept either class. Run both models with the same prompt.\n",
    "class OpenAIModel():\n",
    "    def run(self,prompt):\n",
    "        return f\"OpenAI's response to: '{prompt}'\"\n",
    "\n",
    "class OllamaModel():\n",
    "    def run(self,prompt):\n",
    "        return f\"Ollama's response to: '{prompt}'\"\n",
    "\n",
    "def test_model(model,prompt):\n",
    "    response = model.run(prompt)\n",
    "    print(f\"Result: {response}\")\n",
    "\n",
    "openAIModel = OpenAIModel()\n",
    "ollamaModel = OllamaModel()\n",
    "\n",
    "test_model(openAIModel,\"Explain lists in python.\")\n",
    "test_model(ollamaModel,\"Explain lists in python.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5af8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Write a class `Conversation`: - Stores all user + model messages. - \n",
    "#Has a method to display the full conversation history. - Supports multiple models (OpenAI, Ollama, etc.).\n",
    "\n",
    "class Conversation:\n",
    "   def __init__(self):\n",
    "       self.history = []\n",
    "\n",
    "\n",
    "   def add_message(self, sender, message):\n",
    "       self.history.append((sender, message))\n",
    "\n",
    "\n",
    "   def display(self):\n",
    "       for sender, message in self.history:\n",
    "           print(f\"{sender}: {message}\")\n",
    "\n",
    "\n",
    "conv = Conversation()\n",
    "conv.add_message(\"User\", \"Hello, model\")\n",
    "conv.add_message(\"Mistral\", \"Hi there!\")\n",
    "conv.display()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
