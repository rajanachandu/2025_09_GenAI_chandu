{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72cefeaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chandurajana/Documents/Chandu_Training/2025_09_GenAI_chandu/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "#Run your traced function with 3 different inputs. Capture screenshots of the LangSmith dashboard.\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langsmith import traceable # Need to enable tracing on LangSmith\n",
    "\n",
    "\n",
    "# --- Setup ---\n",
    "load_dotenv(override=True)\n",
    "my_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "langchain_apikey = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "print(langchain_apikey)\n",
    "# Initialize model\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=my_api_key)\n",
    "\n",
    "\n",
    "# --- Function under test ---\n",
    "@traceable # Enable tracing for this function on LangSmith - Only one line change\n",
    "def ask_question(user_prompt) -> str:\n",
    "    \"\"\"LLM function under test.\"\"\"\n",
    "    # If LangSmith passes a dict, extract the actual string\n",
    "    if isinstance(user_prompt, dict):\n",
    "        user_prompt = user_prompt.get(\"input\", \"\")\n",
    "\n",
    "    system_msg = \"You are a helpful assistant. Provide answer in one line.\"\n",
    "    response = llm.invoke(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": system_msg},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "186cb8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Louvre Museum is located in Paris, France.\n",
      "Pakistan is located in South Asia, bordered by India to the east, Afghanistan and Iran to the west, and China to the north.\n",
      "Italy is a country located in Southern Europe, bordered by the Mediterranean Sea to the south, France to the northwest, Switzerland and Austria to the north, and Slovenia to the northeast.\n"
     ]
    }
   ],
   "source": [
    "result1=ask_question(\"Where is the mueseum Louvre?\")\n",
    "print(result1)\n",
    "result2=ask_question(\"Where is the Pakistan?\")\n",
    "print(result2)\n",
    "result3=ask_question(\"Where is the Italy?\")\n",
    "print(result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4adb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "#Compare traces for two different models (gpt-4o-mini vs. gpt-3.5-turbo). What differences do you observe?\n",
    "#when compared, b/w two LLM's, turbo took less tokens and less time to compute.\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langsmith import traceable # Need to enable tracing on LangSmith\n",
    "\n",
    "\n",
    "# --- Setup ---\n",
    "load_dotenv(override=True)\n",
    "my_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "langchain_apikey = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "print(langchain_apikey)\n",
    "# Initialize model\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", api_key=my_api_key)\n",
    "\n",
    "\n",
    "# --- Function under test ---\n",
    "@traceable # Enable tracing for this function on LangSmith - Only one line change\n",
    "def ask_question(user_prompt) -> str:\n",
    "    \"\"\"LLM function under test.\"\"\"\n",
    "    # If LangSmith passes a dict, extract the actual string\n",
    "    if isinstance(user_prompt, dict):\n",
    "        user_prompt = user_prompt.get(\"input\", \"\")\n",
    "\n",
    "    system_msg = \"You are a helpful assistant. Provide answer in one line.\"\n",
    "    response = llm.invoke(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": system_msg},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "931a5cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Louvre Museum is located in Paris, France.\n",
      "Pakistan is located in South Asia.\n",
      "Italy is located in Southern Europe.\n"
     ]
    }
   ],
   "source": [
    "result1=ask_question(\"Where is the mueseum Louvre?\")\n",
    "print(result1)\n",
    "result2=ask_question(\"Where is the Pakistan?\")\n",
    "print(result2)\n",
    "result3=ask_question(\"Where is the Italy?\")\n",
    "print(result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1605ad37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chandurajana/Documents/Chandu_Training/2025_09_GenAI_chandu/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n",
      "Ayodhya is located in the northern Indian state of Uttar Pradesh.\n",
      "Kashmir is a region located in northern India, northeastern Pakistan, and western China, known for its stunning landscapes and political disputes.\n"
     ]
    }
   ],
   "source": [
    "#Create a LangSmith project called \"my_llm_experiment\" and run your traced function inside it.\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langsmith import Client, traceable # Need to enable tracing on LangSmith\n",
    "\n",
    "# --- Setup ---\n",
    "load_dotenv(override=True)\n",
    "my_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "langchain_apikey = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "print(langchain_apikey)\n",
    "\n",
    "# Initialize LangSmith client\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"my_llm_experiment\"\n",
    "client = Client()\n",
    "\n",
    "# Initialize model\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=my_api_key)\n",
    "\n",
    "\n",
    "# --- Function under test ---\n",
    "@traceable # Enable tracing for this function on LangSmith - Only one line change\n",
    "def ask_question(user_prompt) -> str:\n",
    "    \"\"\"LLM function under test.\"\"\"\n",
    "    # If LangSmith passes a dict, extract the actual string\n",
    "    if isinstance(user_prompt, dict):\n",
    "        user_prompt = user_prompt.get(\"input\", \"\")\n",
    "\n",
    "    system_msg = \"You are a helpful assistant. Provide answer in one line.\"\n",
    "    response = llm.invoke(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": system_msg},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return response.content\n",
    "\n",
    "result1=ask_question(\"Where is the Ayodhya?\")\n",
    "print(result1)\n",
    "result2=ask_question(\"Where is the Kashmir?\")\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c507ee84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassifierAgent Initial state: {'question': 'Recommend a book about deep learning and summarize it.'}\n",
      "ChatCompletion(id='chatcmpl-CWXak3bxRz2oErEU491DBnc2Jeyd2', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='{\\n  \"book_recommendation\": [\\n    {\\n      \"title\": \"Deep Learning\",\\n      \"authors\": [\"Ian Goodfellow\", \"Yoshua Bengio\", \"Aaron Courville\"],\\n      \"publisher\": \"MIT Press\",\\n      \"year\": 2016,\\n      \"why_top_seller\": \"Widely regarded as the foundational reference in deep learning; comprehensive coverage of theory, models, and learning algorithms.\"\\n    },\\n    {\\n      \"title\": \"Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition\",\\n      \"authors\": [\"Aurélien Géron\"],\\n      \"publisher\": \"O\\'Reilly Media\",\\n      \"year\": 2019,\\n      \"why_top_seller\": \"Popular practical guide that blends theory with hands-on code and real-world projects, ideal for engineers building DL systems.\"\\n    }\\n  ],\\n  \"summary_info\": [\\n    {\\n      \"title\": \"Deep Learning\",\\n      \"summary\": \"Provides a thorough theoretical foundation for deep learning, covering neural networks, optimization, regularization, convolutional and recurrent architectures, unsupervised pretraining, and theoretical perspectives. Emphasizes understanding of when and why methods work.\"\\n    },\\n    {\\n      \"title\": \"Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition\",\\n      \"summary\": \"A practical, project-driven tour of modern ML and DL workflows. Introduces data preparation, model selection, training techniques, and deployment, with tutorials in Python using popular libraries.\"\\n    }\\n  ]\\n}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1761871658, model='gpt-5-nano-2025-08-07', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=2316, prompt_tokens=68, total_tokens=2384, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=1984, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "Raw response: {\n",
      "  \"book_recommendation\": [\n",
      "    {\n",
      "      \"title\": \"Deep Learning\",\n",
      "      \"authors\": [\"Ian Goodfellow\", \"Yoshua Bengio\", \"Aaron Courville\"],\n",
      "      \"publisher\": \"MIT Press\",\n",
      "      \"year\": 2016,\n",
      "      \"why_top_seller\": \"Widely regarded as the foundational reference in deep learning; comprehensive coverage of theory, models, and learning algorithms.\"\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition\",\n",
      "      \"authors\": [\"Aurélien Géron\"],\n",
      "      \"publisher\": \"O'Reilly Media\",\n",
      "      \"year\": 2019,\n",
      "      \"why_top_seller\": \"Popular practical guide that blends theory with hands-on code and real-world projects, ideal for engineers building DL systems.\"\n",
      "    }\n",
      "  ],\n",
      "  \"summary_info\": [\n",
      "    {\n",
      "      \"title\": \"Deep Learning\",\n",
      "      \"summary\": \"Provides a thorough theoretical foundation for deep learning, covering neural networks, optimization, regularization, convolutional and recurrent architectures, unsupervised pretraining, and theoretical perspectives. Emphasizes understanding of when and why methods work.\"\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition\",\n",
      "      \"summary\": \"A practical, project-driven tour of modern ML and DL workflows. Introduces data preparation, model selection, training techniques, and deployment, with tutorials in Python using popular libraries.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "Parsed response: {'book_recommendation': [{'title': 'Deep Learning', 'authors': ['Ian Goodfellow', 'Yoshua Bengio', 'Aaron Courville'], 'publisher': 'MIT Press', 'year': 2016, 'why_top_seller': 'Widely regarded as the foundational reference in deep learning; comprehensive coverage of theory, models, and learning algorithms.'}, {'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition', 'authors': ['Aurélien Géron'], 'publisher': \"O'Reilly Media\", 'year': 2019, 'why_top_seller': 'Popular practical guide that blends theory with hands-on code and real-world projects, ideal for engineers building DL systems.'}], 'summary_info': [{'title': 'Deep Learning', 'summary': 'Provides a thorough theoretical foundation for deep learning, covering neural networks, optimization, regularization, convolutional and recurrent architectures, unsupervised pretraining, and theoretical perspectives. Emphasizes understanding of when and why methods work.'}, {'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition', 'summary': 'A practical, project-driven tour of modern ML and DL workflows. Introduces data preparation, model selection, training techniques, and deployment, with tutorials in Python using popular libraries.'}]}\n",
      "summaryAgent Initial state: {'question': 'Recommend a book about deep learning and summarize it.', 'book_recommendation': [{'title': 'Deep Learning', 'authors': ['Ian Goodfellow', 'Yoshua Bengio', 'Aaron Courville'], 'publisher': 'MIT Press', 'year': 2016, 'why_top_seller': 'Widely regarded as the foundational reference in deep learning; comprehensive coverage of theory, models, and learning algorithms.'}, {'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition', 'authors': ['Aurélien Géron'], 'publisher': \"O'Reilly Media\", 'year': 2019, 'why_top_seller': 'Popular practical guide that blends theory with hands-on code and real-world projects, ideal for engineers building DL systems.'}]}\n",
      "recommendationAgent Initial state: {'question': 'Recommend a book about deep learning and summarize it.', 'book_recommendation': [{'title': 'Deep Learning', 'authors': ['Ian Goodfellow', 'Yoshua Bengio', 'Aaron Courville'], 'publisher': 'MIT Press', 'year': 2016, 'why_top_seller': 'Widely regarded as the foundational reference in deep learning; comprehensive coverage of theory, models, and learning algorithms.'}, {'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition', 'authors': ['Aurélien Géron'], 'publisher': \"O'Reilly Media\", 'year': 2019, 'why_top_seller': 'Popular practical guide that blends theory with hands-on code and real-world projects, ideal for engineers building DL systems.'}]}\n",
      "ResponseAgent Initial state: {'question': 'Recommend a book about deep learning and summarize it.', 'book_recommendation': [{'title': 'Deep Learning', 'authors': ['Ian Goodfellow', 'Yoshua Bengio', 'Aaron Courville'], 'publisher': 'MIT Press', 'year': 2016, 'why_top_seller': 'Widely regarded as the foundational reference in deep learning; comprehensive coverage of theory, models, and learning algorithms.'}, {'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition', 'authors': ['Aurélien Géron'], 'publisher': \"O'Reilly Media\", 'year': 2019, 'why_top_seller': 'Popular practical guide that blends theory with hands-on code and real-world projects, ideal for engineers building DL systems.'}]}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'summary_info'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 120\u001b[39m\n\u001b[32m    116\u001b[39m builder.add_edge(\u001b[33m\"\u001b[39m\u001b[33mResponseAgent\u001b[39m\u001b[33m\"\u001b[39m, END)\n\u001b[32m    118\u001b[39m graph = builder.compile()\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m result = \u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquestion\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mRecommend a book about deep learning and summarize it.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Final Answer ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    122\u001b[39m \u001b[38;5;28mprint\u001b[39m(result[\u001b[33m\"\u001b[39m\u001b[33mfinal_answer\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Chandu_Training/2025_09_GenAI_chandu/.venv/lib/python3.12/site-packages/langgraph/pregel/main.py:3094\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3091\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3092\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3094\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3095\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3096\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3097\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3098\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   3099\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   3100\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3101\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3102\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3103\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3104\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3105\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3106\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3107\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3108\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   3109\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Chandu_Training/2025_09_GenAI_chandu/.venv/lib/python3.12/site-packages/langgraph/pregel/main.py:2679\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2677\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2678\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2679\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2680\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2681\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2682\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2683\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2684\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2685\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2686\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2687\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2688\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2689\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Chandu_Training/2025_09_GenAI_chandu/.venv/lib/python3.12/site-packages/langgraph/pregel/_runner.py:167\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    165\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Chandu_Training/2025_09_GenAI_chandu/.venv/lib/python3.12/site-packages/langgraph/pregel/_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Chandu_Training/2025_09_GenAI_chandu/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:656\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    654\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    655\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m656\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    658\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Chandu_Training/2025_09_GenAI_chandu/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:400\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    398\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 85\u001b[39m, in \u001b[36mResponseAgent\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;129m@traceable\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mResponseAgent\u001b[39m(state: LibraryState):\n\u001b[32m     79\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mResponseAgent Initial state: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     81\u001b[39m     message_to_llm = [\n\u001b[32m     82\u001b[39m         {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m'''\u001b[39m\u001b[33mYou are a response builder for the library application.\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[33m          Please combine the book recommendation answer and summary info into a coherent response to the user\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms question.\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[33m        \u001b[39m\u001b[33m'''\u001b[39m},\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m         {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mrecommendation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstate[\u001b[33m'\u001b[39m\u001b[33mbook_recommendation\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33msummary Info: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msummary_info\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mQuestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstate[\u001b[33m'\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m}\n\u001b[32m     86\u001b[39m     ]\n\u001b[32m     88\u001b[39m     \u001b[38;5;66;03m# Call the OpenAI model\u001b[39;00m\n\u001b[32m     89\u001b[39m     response = client.chat.completions.create(\n\u001b[32m     90\u001b[39m         model=\u001b[33m\"\u001b[39m\u001b[33mgpt-4.1-nano\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     91\u001b[39m         messages=message_to_llm,\n\u001b[32m     92\u001b[39m         \u001b[38;5;66;03m# temperature=0.2,   # keep it deterministic for classification\u001b[39;00m\n\u001b[32m     93\u001b[39m         \u001b[38;5;66;03m# max_tokens=150,\u001b[39;00m\n\u001b[32m     94\u001b[39m     )\n",
      "\u001b[31mKeyError\u001b[39m: 'summary_info'",
      "During task with name 'ResponseAgent' and id 'cff9a9ee-f659-b061-9948-b8234bcf4aca'"
     ]
    }
   ],
   "source": [
    "#How would you capture and trace an error in your LLM function (e.g., invalid input)? Show an example.\n",
    "#2.Build a Simple Agentic AI App using LangGraph\n",
    "from typing import TypedDict, Optional\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(override=True)\n",
    "my_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "# print (f'Key is {my_api_key}')\n",
    "\n",
    "\n",
    "client = OpenAI(api_key=my_api_key)\n",
    "\n",
    "\n",
    "# --- Shared State ---\n",
    "class LibraryState(TypedDict):\n",
    "    question: Optional[str]\n",
    "    book_recommendation: Optional[str]\n",
    "    summa_info: Optional[str]\n",
    "    final_answer: Optional[str]\n",
    "\n",
    "@traceable\n",
    "def ClassifierAgent(state: LibraryState):\n",
    "    \n",
    "    print(f\"ClassifierAgent Initial state: {state}\")\n",
    "\n",
    "    # Build the LLM messages\n",
    "    message_to_llm = [\n",
    "        {\"role\": \"system\", \"content\": '''You are a helpful librarian that recommends 2 top seller based on \n",
    "        topic. Decide if the user is asking about book info or summary info. \n",
    "        Reply with JSON containing keys: book_recommendation and summary_info.'''},\n",
    "        {\"role\": \"user\", \"content\": f\"Question: {state['question']}\"}\n",
    "    ]\n",
    "    # Call the OpenAI model\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-5-nano\",\n",
    "        messages=message_to_llm,\n",
    "        # temperature=0.2,   # keep it deterministic for classification\n",
    "        # max_tokens=150,\n",
    "    )\n",
    "    print (response)\n",
    "    # Extract the content from the response\n",
    "    answer = response.choices[0].message.content\n",
    "    # Ideally, parse as JSON — here assuming model returns a dict-like string\n",
    "    try:\n",
    "        import json\n",
    "        parsed = json.loads(answer)\n",
    "        print(f\"Raw response: {answer}\")\n",
    "        print(f\"Parsed response: {parsed}\")\n",
    "\n",
    "        return {\n",
    "            \"book_recommendation\": parsed.get(\"book_recommendation\", \"\"),\n",
    "            \"summary_info\": parsed.get(\"summary_info\", \"\")\n",
    "        }\n",
    "    except Exception:\n",
    "        # fallback if LLM gives plain text\n",
    "        return {\"book_recommendation\": answer, \"summary_info\": \"\"}\n",
    "\n",
    "@traceable\n",
    "def recommendationAgent(state: LibraryState):\n",
    "    \n",
    "    print(f\"recommendationAgent Initial state: {state}\")\n",
    "    if not state.get(\"book_recommendation\"):\n",
    "        return {\"book_recommendation\": \"book recommendation: Not requested\"}\n",
    "    return {\"book_recommendation\": state[\"book_recommendation\"]}\n",
    "\n",
    "@traceable\n",
    "def summaryAgent(state: LibraryState):\n",
    "    print(f\"summaryAgent Initial state: {state}\")\n",
    "    if not state.get(\"summary_info\"):\n",
    "        return {\"summary_info\": \"summary info: Not requested\"}\n",
    "    return { \"summary_info\": state[\"summary_info\"]}\n",
    "\n",
    "@traceable\n",
    "def ResponseAgent(state: LibraryState):\n",
    "    print(f\"ResponseAgent Initial state: {state}\")\n",
    "  \n",
    "    message_to_llm = [\n",
    "        {\"role\": \"system\", \"content\": '''You are a response builder for the library application.\n",
    "          Please combine the book recommendation answer and summary info into a coherent response to the user's question.\n",
    "        '''},\n",
    "        {\"role\": \"user\", \"content\": f\"recommendation: {state['book_recommendation']}\\nsummary Info: {state['summary_info']}\\nQuestion: {state['question']}\"}\n",
    "    ]\n",
    "\n",
    "    # Call the OpenAI model\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4.1-nano\",\n",
    "        messages=message_to_llm,\n",
    "        # temperature=0.2,   # keep it deterministic for classification\n",
    "        # max_tokens=150,\n",
    "    )\n",
    "\n",
    "    print (response)\n",
    "    # Extract the content from the response\n",
    "    final_answer = response.choices[0].message.content\n",
    "\n",
    "\n",
    "    return {\"final_answer\": final_answer}\n",
    "\n",
    "\n",
    "# --- Build the Graph ---\n",
    "builder = StateGraph(LibraryState)\n",
    "builder.add_node(\"ClassifierAgent\", ClassifierAgent)\n",
    "builder.add_node(\"recommendationAgent\", recommendationAgent)\n",
    "builder.add_node(\"summaryAgent\", summaryAgent)\n",
    "builder.add_node(\"ResponseAgent\", ResponseAgent)\n",
    "\n",
    "builder.add_edge(START, \"ClassifierAgent\")\n",
    "builder.add_edge(\"ClassifierAgent\", \"recommendationAgent\")\n",
    "builder.add_edge(\"ClassifierAgent\", \"summaryAgent\")\n",
    "builder.add_edge(\"recommendationAgent\", \"ResponseAgent\")\n",
    "builder.add_edge(\"summaryAgent\", \"ResponseAgent\")\n",
    "builder.add_edge(\"ResponseAgent\", END)\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "result = graph.invoke({\"question\": \"Recommend a book about deep learning and summarize it.\"})\n",
    "print(\"\\n--- Final Answer ---\")\n",
    "print(result[\"final_answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
