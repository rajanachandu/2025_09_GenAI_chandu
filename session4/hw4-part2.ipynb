{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fd2096e",
   "metadata": {},
   "source": [
    "1.What is the main purpose of RAG?\n",
    "    The main purpose of RAG (Retrieval-Augmented Generation) is to make AI models give more accurate, up-to-date, and reliable answers by letting them look up external information before generating a response.\n",
    "    \n",
    "    Normally, an AI model can only answer based on what it was trained on (its â€œmemoryâ€).\n",
    "    But RAG adds a retrieval step â€” it searches through real documents, databases, or the web before answering.\n",
    "    \n",
    "    Itâ€™s not an AI model and not exactly an AI agent â€” itâ€™s a technique or architecture used to make language models (like GPT, Llama, or Mistral) smarter and more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56377242",
   "metadata": {},
   "source": [
    "2.How is RAG different from traditional question-answering systems?\n",
    "\n",
    "| Feature                         | Traditional QA                   | RAG                                             |\n",
    "| ------------------------------- | -------------------------------- | ----------------------------------------------- |\n",
    "| **Core idea**                   | Retrieve and extract             | Retrieve and generate                           |\n",
    "| **Uses LLM?**                   | Usually no                       | Yes                                             |\n",
    "| **Answer style**                | Factual, short, literal          | Natural, contextual, detailed                   |\n",
    "| **Handles multiple documents?** | Harder                           | Yes, easily                                     |\n",
    "| **Can reason or summarize?**    | Limited                          | Yes                                             |\n",
    "| **Data freshness**              | From database                    | From retrieved sources (can be updated anytime) |\n",
    "| **Example tech**                | Elasticsearch + rule-based model | Vector DB + LLM (e.g., Llama, Mistral)          |\n",
    "\n",
    "ğŸ§© In simple terms:\n",
    "Traditional QA: Looks up and copies answers.\n",
    "RAG: Looks up, understands, and writes answers in its own words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82edf988",
   "metadata": {},
   "source": [
    "3.Why do we use embeddings in a RAG pipeline?\n",
    "\n",
    "In a RAG (Retrieval-Augmented Generation) system, embeddings are used to find the most relevant pieces of information from a large set of documents before giving them to the AI model.\n",
    "\n",
    "They are the bridge between your text data and the AIâ€™s understanding of that data.\n",
    "ğŸ” Step-by-step Explanation\n",
    "1. Convert text into embeddings\n",
    "An embedding is a list of numbers (a vector) that represents the meaning of a piece of text.\n",
    "Example:\n",
    "â€œcarâ€ and â€œautomobileâ€ â†’ very similar embeddings (close in meaning).\n",
    "â€œcarâ€ and â€œbananaâ€ â†’ very different embeddings.\n",
    "2. Store embeddings in a vector database\n",
    "Every document or text chunk is stored with its embedding.\n",
    "So later, when you ask a question, you can search by meaning, not by exact words.\n",
    "3. Retrieve relevant info\n",
    "When the user asks a question, that question is also turned into an embedding.\n",
    "The system compares it with stored embeddings to find the closest matches (semantically similar texts).\n",
    "These matches are then sent to the LLM as context for generating the answer.\n",
    "\n",
    "Example:\n",
    "You store:\n",
    "â€œPython is a programming language.â€\n",
    "â€œSnakes can be dangerous.â€\n",
    "Then you ask:\n",
    "â€œTell me about coding in Python.â€\n",
    "Without embeddings â†’ might match both because they contain the word â€œPython.â€\n",
    "With embeddings â†’ matches only the programming one, because the meaning is similar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f367285d",
   "metadata": {},
   "source": [
    "4.What role does a vector database like Milvus play in RAG?\n",
    "A vector database such as Milvus, Pinecone, Weaviate, or FAISS is used to store and search embeddings efficiently.\n",
    "Itâ€™s basically the â€œmemoryâ€ or â€œsearch engineâ€ for your RAG system â€” but instead of matching keywords, it matches meanings.\n",
    "\n",
    "Hereâ€™s what happens step-by-step:\n",
    "1.Chunk and Embed Documents, You split your documents (PDFs, articles, notes) into smaller chunks.\n",
    "2.Each chunk is converted into a vector embedding (a numerical representation of meaning). Store in Vector Database (like Milvus)\n",
    "You store these embeddings in Milvus.\n",
    "3.Milvus organizes them so it can quickly find which ones are most similar to a query embedding.\n",
    "This is called vector similarity search.\n",
    "4.Query Time â€” Retrieve Relevant Chunks\n",
    "When a user asks a question, the system converts the question into an embedding too.\n",
    "Milvus searches for embeddings (document chunks) that are closest in meaning to that question.\n",
    "It returns those top results (e.g., top 3â€“5 text chunks).\n",
    "5.Augment the Prompt for the LLM\n",
    "The retrieved chunks are added to the userâ€™s question and passed to the LLM (like Llama, Mistral, GPT).\n",
    "The LLM then generates an informed, context-rich answer.\n",
    "\n",
    "\n",
    "5.Explain the flow of a RAG pipeline in your own words (from user query â†’ retrieved docs â†’ LLM response).\n",
    "\n",
    "\n",
    "                   ğŸ”¹ RAG Pipeline with Milvus ğŸ”¹\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                         USER                             â”‚\n",
    "â”‚        \"What are the benefits of using Python?\"           â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                â”‚\n",
    "                                â–¼\n",
    "                     ğŸ§  1ï¸âƒ£  Create Query Embedding\n",
    "                      (turn question â†’ vector)\n",
    "                                â”‚\n",
    "                                â–¼\n",
    "       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "       â”‚         2ï¸âƒ£  Vector Database (Milvus)        â”‚\n",
    "       â”‚   - Stores document embeddings               â”‚\n",
    "       â”‚   - Finds similar vectors (semantic search)  â”‚\n",
    "       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                â”‚\n",
    "                ğŸ” Returns top relevant chunks\n",
    "                                â”‚\n",
    "                                â–¼\n",
    "     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "     â”‚      3ï¸âƒ£  Augment Prompt with Retrieved Text  â”‚\n",
    "     â”‚   e.g., \"According to docs: [chunk1] [chunk2]\"â”‚\n",
    "     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                â”‚\n",
    "                                â–¼\n",
    "            ğŸ¤– 4ï¸âƒ£  Large Language Model (e.g., Llama, Mistral)\n",
    "                 - Reads question + retrieved context\n",
    "                 - Generates final answer\n",
    "                                â”‚\n",
    "                                â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                         ANSWER                           â”‚\n",
    "â”‚   \"Python is easy to learn, has many libraries, and is    â”‚\n",
    "â”‚    widely used for data science and web development.\"     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25bab8d",
   "metadata": {},
   "source": [
    "6.What are some real-world scenarios where RAG is more effective than fine-tuning?\n",
    "RAG is often more effective than fine-tuning in scenarios where information changes frequently, is large, or private.\n",
    "\n",
    "Fine-tuning trains the model on specific data, making it â€œspecialized,â€ but it is static and costly to update.\n",
    "\n",
    "RAG retrieves information at query time, so it always uses fresh, relevant data without retraining the model.\n",
    "\n",
    "âš¡ï¸ Real-World Scenarios Where RAG Excels\n",
    "1ï¸âƒ£ Frequently Changing Information\n",
    "Example: Stock prices, weather updates, product catalogs, or news.\n",
    "Why RAG wins: You can pull the latest data live. Fine-tuning would need constant retraining to stay accurate.\n",
    "2ï¸âƒ£ Large or Expanding Knowledge Bases\n",
    "Example: Company manuals, legal documents, research papers.\n",
    "Why RAG wins: You can index millions of documents in a vector database. Fine-tuning a model on all this would be expensive or impractical.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Fine-tuning\n",
    "-----------\n",
    "Fine-tuning is the process of teaching an existing AI model new or specialized knowledge by training it further on specific examples or data.\n",
    "\n",
    "RAG & Fine-tuning together\n",
    "--------------------------\n",
    "How They Work Together\n",
    "1.Fine-Tune the Base Model\n",
    "Teach it your domain style and behavior.\n",
    "Example: make it sound professional, follow your workflow, or summarize technical docs in a certain format.\n",
    "2.Add a RAG Layer on Top\n",
    "Connect it to your document or knowledge base.\n",
    "When a user asks something, RAG retrieves the relevant info and gives it to the fine-tuned model.\n",
    "3.Generate an Answer\n",
    "The fine-tuned model uses its learned style and the fresh facts from RAG to produce a precise, on-brand answer."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
