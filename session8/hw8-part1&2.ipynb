{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc8fb6b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    }
   ],
   "source": [
    "#1.Write a function square_sum(numbers) that takes a list of integers and returns the sum of their squares.\n",
    "\n",
    "def square_sum(list_num):\n",
    "    sum_nums = 0\n",
    "    for i in list_num:\n",
    "        sqr = i*i\n",
    "        sum_nums=sum_nums+sqr\n",
    "    return sum_nums\n",
    "\n",
    "\n",
    "sum_of_sqrs = square_sum([1,2,3,4,5])\n",
    "print(sum_of_sqrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48f494fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alex\n",
      "['Full stack', 'ETL']\n",
      "['Full stack', 'ETL', 'LangChain']\n"
     ]
    }
   ],
   "source": [
    "#2.Create a dictionary with keys name, age, skills and add 'LangChain' to the skills list.\n",
    "\n",
    "candidate = {\n",
    "    'name':'Alex',\n",
    "    'age':30,\n",
    "    'skills':['Full stack','ETL']\n",
    "}\n",
    "print(candidate.get(\"name\"))\n",
    "print(candidate.get(\"skills\"))\n",
    "\n",
    "candidate['skills'].append('LangChain')\n",
    "print(candidate.get(\"skills\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9833d122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alex\n",
      "Alex Ram\n"
     ]
    }
   ],
   "source": [
    "#3. Create a class PromptTemplate with method format_prompt(name) to replace {name}.\n",
    "class PromptTemplate:\n",
    "    def __init__(self,name):\n",
    "        self.name=name\n",
    "        print(name)\n",
    "\n",
    "    def format_prompt(self,name):\n",
    "        self.name=name\n",
    "        print(name)\n",
    "\n",
    "alex = PromptTemplate(\"Alex\")\n",
    "alex.format_prompt(\"Alex Ram\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d086422d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading content:\n",
      " LangChain is awesome!\n",
      "LangChain is awesome!\n",
      "Text appended successfully!\n"
     ]
    }
   ],
   "source": [
    "#4. File I/O: Read and append 'LangChain is awesome!' to concept.txt file.\n",
    "#'r' → read existing\n",
    "#'a' → append to file\n",
    "#'w' → overwrite file completely\n",
    "def Read_append(file_path,text):\n",
    "    try:\n",
    "        # Read the existing content\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "            print(\"Reading content:\\n\", content)\n",
    "    except FileNotFoundError:\n",
    "            print(f\"Error: The file '{file_path}' does not exist.\")\n",
    "            return None\n",
    "\n",
    "    # Append new text\n",
    "    with open(file_path, 'a', encoding='utf-8') as file:\n",
    "        file.write('\\n'+text)\n",
    "        print(\"Text appended successfully!\")\n",
    "\n",
    "\n",
    "text = 'LangChain is awesome!'\n",
    "file_path = \"/Users/chandurajana/Documents/Chandu_Training/2025_09_GenAI_chandu/session8/concept.txt\" \n",
    "Read_append(file_path,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d7697a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 1, 2: 4, 3: 9, 4: 16, 5: 25, 6: 36, 7: 49, 8: 64, 9: 81, 10: 100}\n",
      "{1: 1, 2: 4, 3: 9, 4: 16, 5: 25, 6: 36, 7: 49, 8: 64, 9: 81, 10: 100}\n"
     ]
    }
   ],
   "source": [
    "#5.Generate dictionary of squares for 1-10.\n",
    "sqr_dict = {}\n",
    "\n",
    "for i in range(1,11):\n",
    "    sqr = i*i\n",
    "    sqr_dict[i] = sqr\n",
    "\n",
    "print(sqr_dict)\n",
    "\n",
    "squares = {i:i*i for i in range(1,11)}\n",
    "print(squares)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2df8383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='LangChain is a framework for building language-model-powered applications by chaining prompts, memory, agents, tools, and data sources into modular components.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 293, 'prompt_tokens': 27, 'total_tokens': 320, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 256, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CW9L9fCyTnjaH31TVy0bKi3A3tTlc', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--7a73a8cc-963c-4200-9e10-a90d8ad586e5-0' usage_metadata={'input_tokens': 27, 'output_tokens': 293, 'total_tokens': 320, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 256}}\n"
     ]
    }
   ],
   "source": [
    "#6.Your First Chain: Use LLMChain and PromptTemplate to answer 'Explain LangChain in one line.'\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# --- Load API Key ---\n",
    "load_dotenv(override=True, dotenv_path=\"../.env\")\n",
    "my_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "# from langchain_ollama import Ollama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Choose the LLM\n",
    "my_llm = ChatOpenAI(model=\"gpt-5-nano\", openai_api_key=my_api_key)\n",
    "\n",
    "# llm_ollama = Ollama(model=\"llama2\", base_url=\"http://localhost:11434\")\n",
    "\n",
    "# Create a prompt template\n",
    "template = \"\"\"You are an AI asstant. Respond this concept in one line: {concept}\"\"\"\n",
    "my_prompt = PromptTemplate(template=template, input_variables=[\"concept\"])\n",
    "\n",
    "# Build the chain\n",
    "#chain = LLMChain(llm=my_llm, prompt=my_prompt)\n",
    "# this replaces LLMChain\n",
    "chain = my_prompt | my_llm\n",
    "\n",
    "# Run the chain\n",
    "#response = chain.run(\"What is Retrieval Augmented Generation?\")\n",
    "response = chain.invoke({\"concept\": \"Explain LangChain in one line.\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ddebfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chandurajana/Documents/Chandu_Training/2025_09_GenAI_chandu/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: content='Got it, Chandu — I’ll remember your name for this conversation. How can I help you today?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 287, 'prompt_tokens': 15, 'total_tokens': 302, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 256, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-mini-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CW9KRfYNdOB6klpZS61ie0CQ9Zcy3', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--7e47808c-e615-4ba9-8e14-daa674d227bb-0' usage_metadata={'input_tokens': 15, 'output_tokens': 287, 'total_tokens': 302, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 256}}\n",
      "AI: content='Your name is Chandu.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 79, 'prompt_tokens': 52, 'total_tokens': 131, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 64, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-mini-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CW9KWuyEZTG48394LDlF7rW4oo6QH', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--aa6643b3-05de-49eb-a441-679c6cb53a3a-0' usage_metadata={'input_tokens': 52, 'output_tokens': 79, 'total_tokens': 131, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 64}}\n",
      "AI: content='LangChain is an open-source framework for building LLM-powered applications by composing prompts, models, memory, agents, and tools into reusable chains and pipelines.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 168, 'prompt_tokens': 75, 'total_tokens': 243, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 128, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-mini-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CW9KZpfhUGAO2hviTLE27xQljXz4q', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--86a001d2-8aae-4584-adfc-a38b3ce038a1-0' usage_metadata={'input_tokens': 75, 'output_tokens': 168, 'total_tokens': 243, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 128}}\n",
      "AI: content='LLM application framework' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 141, 'prompt_tokens': 125, 'total_tokens': 266, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 128, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-mini-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CW9Kd3NU15GRHrplWsF6JAWmlB4Hw', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--0add1244-7d35-4361-b138-ffb2e7438da3-0' usage_metadata={'input_tokens': 125, 'output_tokens': 141, 'total_tokens': 266, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 128}}\n"
     ]
    }
   ],
   "source": [
    "#7.Adding Memory: Use ConversationChain and ConversationBufferMemory to test conversation recall.\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# 1) LLM\n",
    "llm = ChatOpenAI(model=\"gpt-5-mini\", temperature=0)\n",
    "\n",
    "# 2) In-memory storage for sessions (dict of histories)\n",
    "store = {}\n",
    "\n",
    "# 3) Required callback: returns the right memory per session\n",
    "def get_session_history(session_id: str):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# 4) Wrap LLM with history\n",
    "conversation = RunnableWithMessageHistory(\n",
    "    llm,\n",
    "    get_session_history=get_session_history,\n",
    ")\n",
    "\n",
    "# ---- Talk using a session_id e.g. \"user1\" ----\n",
    "\n",
    "reply1 = conversation.invoke(\n",
    "    \"My name is Chandu. Remember it.\",\n",
    "    config={\"configurable\": {\"session_id\": \"user1\"}}\n",
    ")\n",
    "print(\"AI:\", reply1)\n",
    "\n",
    "reply2 = conversation.invoke(\n",
    "    \"What is my name?\",\n",
    "    config={\"configurable\": {\"session_id\": \"user1\"}}\n",
    ")\n",
    "print(\"AI:\", reply2)\n",
    "\n",
    "reply3 = conversation.invoke(\n",
    "    \"Explain LangChain in one line.\",\n",
    "    config={\"configurable\": {\"session_id\": \"user1\"}}\n",
    ")\n",
    "print(\"AI:\", reply3)\n",
    "\n",
    "reply4 = conversation.invoke(\n",
    "    \"Summarize that in 3 words.\",\n",
    "    config={\"configurable\": {\"session_id\": \"user1\"}}\n",
    ")\n",
    "print(\"AI:\", reply4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "769a9dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='In Python, ArithmeticError is the base exception for arithmetic-related errors (e.g., ZeroDivisionError, OverflowError, FloatingPointError).' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 294, 'prompt_tokens': 24, 'total_tokens': 318, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 256, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CW9RPCyEvTiNEYoxeNvJh2drBjxFw', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--57eeef7f-c08f-44f4-91d3-a58a09e75a09-0' usage_metadata={'input_tokens': 24, 'output_tokens': 294, 'total_tokens': 318, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 256}}\n"
     ]
    }
   ],
   "source": [
    "#8.Custom Prompt: Build a prompt to explain {topic} in simple terms.\n",
    "my_llm = ChatOpenAI(model=\"gpt-5-nano\", openai_api_key=my_api_key)\n",
    "template = \"\"\"You are an AI asstant. Respond this concept in one line: {summary}\"\"\"\n",
    "my_prompt = PromptTemplate(template=template, input_variables=[\"summary\"])\n",
    "\n",
    "# Build the chain\n",
    "#chain = LLMChain(llm=my_llm, prompt=my_prompt)\n",
    "# this replaces LLMChain\n",
    "chain = my_prompt | my_llm\n",
    "\n",
    "# Run the chain\n",
    "#response = chain.run(\"What is Retrieval Augmented Generation?\")\n",
    "response = chain.invoke({\"summary\": \"What is ArithmeticError\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a876a050",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9.Explore Docs: Summarize what a Chain is in LangChain.\n",
    "#Chains link multiple components like LLMs, prompts, retrievers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4de904c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain langchain-community pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35d35775",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_core.text_splitter'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#10.Try a from langchain.document_loaders import TextLoader\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdocument_loaders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PyPDFLoader\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtext_splitter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_openai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAIEmbeddings, ChatOpenAI\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvectorstores\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Milvus\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain_core.text_splitter'"
     ]
    }
   ],
   "source": [
    "#10.Try a from langchain.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_core.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import Milvus\n",
    "from langchain_core.chains import RetrievalQA\n",
    "\n",
    "# 2) Load documents\n",
    "# (Replace with your local text files)\n",
    "loader = TextLoader(\"example.txt\", encoding=\"utf-8\")\n",
    "docs = loader.load()\n",
    "\n",
    "# 3) Split into chunks\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "split_docs = splitter.split_documents(docs)\n",
    "\n",
    "# 4) Create embeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# 5) Connect to Milvus vectorstore\n",
    "vectorstore = Milvus.from_documents(\n",
    "    split_docs,\n",
    "    embeddings,\n",
    "    connection_args={\"host\": \"localhost\", \"port\": \"19530\"},\n",
    "    collection_name=\"rag_collection\"\n",
    ")\n",
    "\n",
    "# 6) Create Retriever\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# 7) LLM\n",
    "llm = ChatOpenAI(model=\"gpt-5-mini\", temperature=0)\n",
    "\n",
    "# 8) RetrievalQA chain\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\"  # simple approach: stuff all retrieved docs into prompt\n",
    ")\n",
    "\n",
    "# 9) Ask a question\n",
    "query = \"Explain the main idea from the text.\"\n",
    "answer = qa.run(query)\n",
    "print(\"Answer:\", answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
